---
title: "final project"
author: "Yixin Zheng"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(skimr)
library(dplyr)
library(ggplot2)
library(caret)
library(corrplot)
library(lsr)
library(vcd)
library(car)
library(gridExtra)
library(robustbase)

```

## Data Cleaning and Preprocessing
```{r data import and initial tidy}
# Import data and clean column names
data <- read.csv("./data/Project_2_data.csv") %>% 
  clean_names()

# Select relevant covariates (variables 1-14) and outcome variable
model_data <- data %>%
  select(
    age, race, marital_status, t_stage, n_stage, x6th_stage, differentiate, grade,
    a_stage, tumor_size, estrogen_status, progesterone_status,
    regional_node_examined, reginol_node_positive, status
  )

# Convert categorical variables to factors and relabel `grade`
model_data <- model_data %>%
  mutate(
    race = factor(race),
    marital_status = factor(marital_status),
    t_stage = factor(t_stage),
    n_stage = factor(n_stage),
    x6th_stage = factor(x6th_stage),
    differentiate = factor(differentiate),
    a_stage = factor(a_stage),
    estrogen_status = factor(estrogen_status),
    progesterone_status = factor(progesterone_status),
    status = factor(status, levels = c("Alive", "Dead")),
    grade = case_when(
      grade == "1" ~ "1",
      grade == "2" ~ "2",
      grade == "3" ~ "3",
      grade == " anaplastic; Grade IV" ~ "4",
      TRUE ~ NA_character_
    ) %>% factor(levels = c("1", "2", "3", "4"))
  )

# Summarize structure of the cleaned dataset
summary(model_data)
```
## Exploratory Data Analysis
### Summary statistics
```{r summary statistics}
# Summary statistics for continuous and categorical variables
skim(model_data)

# Key statistics grouped by survival status
summary_by_status <- model_data %>%
  group_by(status) %>%
  summarise(
    mean_age = mean(age, na.rm = TRUE),
    sd_age = sd(age, na.rm = TRUE),
    mean_tumor_size = mean(tumor_size, na.rm = TRUE),
    sd_tumor_size = sd(tumor_size, na.rm = TRUE),
    prop_white = mean(race == "White", na.rm = TRUE),
    prop_black_other = mean(race != "White", na.rm = TRUE),
    n_obs = n()
  )
print(summary_by_status)
```

### Distribution Visualization

#### Categorical Variables
Our modified dataset contains **10 categorical variables**:  
- **race**: Patient’s race (Black, White, Other).  
- **marital_status**: Patient’s marital status (Divorced, Married, Separated, Single, Widowed).  
- **t_stage**: Tumor stage (T1, T2, T3, T4). ("T" refers to the size of the primary tumor)
- **n_stage**: Lymph node stage (N1, N2, N3). (extent of cancer spread to nearby lymph nodes)
- **x6th_stage**: Adjusted AJCC 6th stage (IIA, IIB, IIIA, IIIB, IIIC).  
- **differentiate**: Tumor differentiation (Well, Moderately, Poorly, Undifferentiated).  
- **grade**: Grade of the tumor (1–4). 
- **a_stage**: Tumor spread stage (Regional, Distant).  
- **estrogen_status**: Estrogen receptor status (Positive, Negative).  
- **progesterone_status**: Progesterone receptor status (Positive, Negative).  

To examine the association of these variables with the binary outcome `status` (Alive/Dead), I used **Cramér's V**, which quantifies the strength of the association between two categorical variables based on the Chi-Square statistic. 
Cramér's V ranges from **0** (no association) to **1** (perfect association). This method helps identify the predictors most strongly associated with survival status, enabling us to prioritize variables for modeling.
```{r categorical varibales}
# Define categorical variables to analyze
variables <- c("race", "marital_status", "t_stage", "n_stage", "x6th_stage", 
               "differentiate", "grade", "a_stage", "estrogen_status", "progesterone_status")

# Initialize a vector to store Cramér's V results
results <- numeric(length(variables))

# Calculate Cramér's V for each variable
for (i in seq_along(variables)) {
  var <- variables[i]
  
  # Select outcome and predictor variable, omitting missing values
  df_temp <- model_data %>%
    select(status, all_of(var)) %>%
    na.omit()
   
  # Convert both columns to factors
  x <- droplevels(as.factor(df_temp$status))
  y <- droplevels(as.factor(df_temp[[var]]))
  
  # Create contingency table and calculate Cramér's V
  table_var <- table(x, y)
  results[i] <- cramersV(table_var)
}

# Create a dataframe with results
association_df <- data.frame(Variable = variables, CramersV = results)

# Plot Cramér's V values
ggplot(association_df, aes(x = reorder(Variable, CramersV), y = CramersV, fill = CramersV)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(
    title = "Association Between Survival Status and Predictor Variables",
    x = "Predictor Variables",
    y = "Cramér's V",
    fill = "Cramér's V"
  ) +
  theme_minimal()
```
The plot shows the strength of association (Cramér’s V) between **survival status** and each categorical predictor:

1. **x6th_stage** and **n_stage** have the **highest Cramér's V values** (around 0.25), indicating they are the most informative predictors of survival status in the dataset. These variables reflect tumor stage and lymph node involvement, which are critical factors in breast cancer prognosis.

2. **Estrogen_status**, **progesterone_status**, and **grade** exhibit **moderate associations** (Cramér’s V around 0.15–0.2). These variables provide meaningful information about hormone status and differentiation, making them important contributors to survival prediction.

3. **Differentiate** and **t_stage** show moderate but slightly lower associations compared to the top variables, suggesting their relevance to survival status.

4. **a_stage**, **marital_status**, and **race** have **lower Cramér’s V values** (less than 0.1), indicating weaker associations with survival status. Although these variables contribute limited information individually, they may still be useful in interaction terms or when combined with other predictors.

By focusing on variables with higher Cramér’s V values, we can build more efficient and predictive models for survival status analysis.

```{r race}
# Proportional Bar Plot for Survival Status by Race
ggplot(model_data, aes(x = race, fill = status)) +
  geom_bar(position = "fill", alpha = 0.8, color = "black") +
  scale_fill_manual(values = c("Alive" = "gold", "Dead" = "lightblue")) +
  theme_minimal() +
  labs(
    title = "Survival Status by Race",
    x = "Race",
    y = "Proportion",
    fill = "Status"
  )

# Combine "Black" and "Other" into a single group "Minority Non-White"
model_data_race_combined <- model_data %>%
  mutate(
    race_combined = case_when(
      race == "White" ~ "Majority White",
      race %in% c("Black", "Other") ~ "Minority Non-White"
    ),
    race_combined = factor(race_combined, levels = c("Majority White", "Minority Non-White"))
  )

# Proportional Bar Plot for Combined Race Groups
ggplot(model_data_race_combined, aes(x = race_combined, fill = status)) +
  geom_bar(position = "fill", alpha = 0.8, color = "black") +
  scale_fill_manual(values = c("Alive" = "gold", "Dead" = "lightblue")) +
  theme_minimal() +
  labs(
    title = "Survival Status by Combined Race Groups",
    x = "Race Group",
    y = "Proportion",
    fill = "Status"
  )
```
Both plots confirm a disparity in survival outcomes across racial groups.
Non-White patients, particularly Black patients, show a higher likelihood of death.
Effect of Combining Groups:
Combining Black and Other into Minority Non-White simplifies the comparison and reduce the racial disparities between Majority White and Minority Non-White groups.

#### Continuous Variables 
```{r continuous variables}
continuous_vars <- model_data %>%
  select(age, tumor_size, regional_node_examined, reginol_node_positive) %>%
  na.omit()

df1 <- as.data.frame(continuous_vars)
par(mfrow = c(2, 4))
for (col in names(df1)) {
  hist(df1[[col]], 
       main = paste("Histogram of", col), 
       xlab = col, 
       col = "beige", 
       border = "black")
}
par(mfrow = c(1, 1))
```
Histograms were created for each continuous variable to visually examine their distributions and identify potential skewness. The results revealed that, except for `age`, the other three variables exhibited significant right skewness, indicating the need for transformation if we want to perform linear regression. Though we are not doing linear regression, transformation has still been performed here, we see that after performing log-transformation, there is a improvement on the skewness of `tumor_size`, `regional_node_examined` and `reginol_node_positive`.

```{r transformation}
df_log <- df1 %>%
  select(-tumor_size, -regional_node_examined, -reginol_node_positive) %>%
  mutate(
    tumor_size_log = log(df1$tumor_size + 1),
    rn_examined_log = log(df1$regional_node_examined + 1),
    rn_positive_log = log(df1$reginol_node_positive + 1)
  )

par(mfrow = c(2, 4))
for (col in names(df_log)) {
  hist(
    df_log[[col]],
    main = paste("Histogram of", col),
    xlab = col,
    col = "beige",
    border = "black"
  )
}
par(mfrow = c(1, 1))

```

Boxplots were then generated for each continuous variable stratified by survival status to visually assess their relationships with the binary outcome. 

```{r age}
# Age by survival status
ggplot(model_data, aes(x = status, y = age, fill = status)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = c("Alive" = "gold", "Dead" = "lightblue")) +
  theme_minimal() +
  labs(
    title = "Age by Survival Status",
    x = "Status",
    y = "Age"
  )
```

```{r tumor size}
# Tumor size by survival status
ggplot(model_data, aes(x = status, y = tumor_size, fill = status)) +
  geom_boxplot(alpha = 0.8, color = "black") +
  scale_fill_manual(values = c("Alive" = "gold", "Dead" = "lightblue")) +
  theme_minimal() +
  labs(
    title = "Tumor Size by Survival Status",
    x = "Status",
    y = "Tumor Size (mm)"
  )

# Tumor size by race and survival status
ggplot(model_data, aes(x = race, y = tumor_size, fill = status)) +
  geom_boxplot(alpha = 0.8, color = "black") +
  scale_fill_manual(values = c("Alive" = "gold", "Dead" = "lightblue")) +
  theme_minimal() +
  labs(
    title = "Tumor Size by Race and Survival Status",
    x = "Race",
    y = "Tumor Size (mm)"
  )
```

```{r rn_examined}
# Regional Node Examined by survival status
ggplot(model_data, aes(x = status, y = regional_node_examined, fill = status)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = c("Alive" = "gold", "Dead" = "lightblue")) +
  theme_minimal() +
  labs(
    title = "Regional Node Examined by Survival Status",
    x = "Status",
    y = "regional_node_examined"
  )
```

```{r rn_positive}
# Reginol Node Positive by survival status
ggplot(model_data, aes(x = status, y = reginol_node_positive, fill = status)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = c("Alive" = "gold", "Dead" = "lightblue")) +
  theme_minimal() +
  labs(
    title = "Reginol Node Positive by Survival Status",
    x = "Status",
    y = "regional_node_positive"
  )
```

### Pairwise Relationships and Interactions
```{r pairwise}
correlation_matrix <- cor(continuous_vars, use = "pairwise.complete.obs")

# Pairwise relationships (correlation matrix for continuous variables)
corrplot(correlation_matrix, method = "circle", 
         type = "upper", tl.col = "black", addCoef.col = "grey", 
         number.cex = 0.8, tl.cex = 0.9)
```
A correlation matrix was generated to examine the relationships between the variables.
Small circles (near-zero correlations) are observed for age and all other variables; tumor_size and regional_node_examined suggesting weak or no relationships.
`regional_node_positive` is moderately associated with both `tumor size` (0.24) and `regional nodes examined` (0.41), which might influence modeling decisions.
There are no strong correlations (close to ±1), suggesting no immediate multicollinearity issues among these variables.

## Modeling - Logistic Regression
Since the outcome is Binary (e.g., Alive/Dead), We decide to proceed with logistic regression instead of linear regression.
Logistic regression outputs odds ratios, which are more interpretable for binary classification problems. For example, it tells you how much the odds of death increase with a unit increase in a predictor.

### Checking Assumptions and Transformations
#### Binary or Dichotomous Response Variable
```{r binary response}
unique(model_data$status)
```
The response variable (status) has exactly two categories: Alive and Dead.

#### No Multicollinearity
```{r multicollinearity1}
# Fit an initial logistic regression model
model_data_1 <- model_data %>%
  mutate(
    race = relevel(race, ref = "White"),  # Set "White" as reference
    grade = relevel(grade, ref = "1"),    # Set "Grade 1" as reference
    x6th_stage = relevel(x6th_stage, ref = "IIA") # Set "IIA" as reference
  )

alias(glm(status ~ ., data = model_data_1, family = binomial))
```
From the alias() output:
grade2, grade3, and grade4 are collinear with other predictors.
x6th_stageIIIC is collinear with n_stage or other levels of x6th_stage.
differentiate levels also overlap in prediction with grade.

Given that both `x6th_stage` and `n_stage` are strong predictors, with `x6th_stage` showing a slightly higher association, `n_stage` can be dropped as its information is already captured by `x6th_stage`. Similarly, `t_stage`, which is related to tumor size, can also be removed to avoid redundancy. The variable `differentiate` can be excluded as well, as it overlaps with `grade`, with both variables reflecting tumor differentiation. In the context of breast cancer, "regional node positive" indicates the presence of cancer cells in nearby lymph nodes, while "regional node examined" refers to the surgical removal and analysis of these nodes to determine cancer spread. From the previous correlation matrix, `regional_node_positive` shows a moderate association with both `tumor size` (0.24) and `regional_node_examined` (0.41), suggesting some redundancy. To simplify the model and minimize multicollinearity, we choose to drop `regional_node_positive`.

```{r multicollinearity1}
model_data_2 <- model_data_1 %>%
  select(-differentiate, -n_stage, -t_stage, -reginol_node_positive)

alias(glm(status ~ ., data = model_data_2, family = binomial))

# Fit an initial logistic regression model
model_vif <- glm(status ~ ., data = model_data_2, family = binomial)

# Check VIF for predictors
vif_values <- vif(model_vif)
print(vif_values)

# Identify predictors with VIF > 5 (indicating multicollinearity)
vif_values[vif_values > 5]
```
The results show that all VIF values are below 5, which indicates that multicollinearity is no longer a concern in your logistic regression model.

#### Linear Relationship to Log Odds
```{r Log Odds}
continuous_vars_log_odds <- model_data_2 %>%
  select(age, tumor_size, regional_node_examined, status) %>%  # Include 'status'
  na.omit()

# Log-transform tumor size and regional nodes examined
df_log_odds <- continuous_vars_log_odds %>%
  mutate(
    age_log = log(age + 1),
    tumor_size_log = log(tumor_size + 1),
    rn_examined_log = log(regional_node_examined + 1)
  )

linearity_test <- glm(status ~ age_log + tumor_size_log + rn_examined_log, 
                      data = df_log_odds, 
                      family = binomial)

df_log_odds$logit <- predict(linearity_test, type = "link") 


plot1 <- ggplot(df_log_odds, aes(x = age_log, y = logit)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "loess", color = "purple") +
  labs(title = "Logit vs Age (Log-Transformed)", 
       x = "Log(Age + 1)", y = "Logit (Log Odds)") +
  theme_minimal()

plot2 <- ggplot(df_log_odds, aes(x = tumor_size_log, y = logit)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "loess", color = "purple") +
  labs(title = "Logit vs Tumor Size (Log-Transformed)", x = "Log(Tumor Size + 1)", y = "Logit (Log Odds)") +
  theme_minimal()

plot3 <- ggplot(df_log_odds, aes(x = rn_examined_log, y = logit)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "loess", color = "purple") +
  labs(title = "Logit vs Regional Nodes Examined (Log-Transformed)", 
       x = "Log(Regional Nodes Examined + 1)", y = "Logit (Log Odds)") +
  theme_minimal()

grid.arrange(plot1, plot2, plot3, ncol = 3)

# Update
model_data_3 <- model_data_2 %>%
  mutate(
    tumor_size_log = log(tumor_size + 1),
    rn_examined_log = log(regional_node_examined + 1),
    age_log = log(age + 1)
  ) %>%
  select(-tumor_size, -regional_node_examined, -age) 
```
From the plot, we see that there's a linear relationship between continuous predictors (log transformed) and the log odds of the outcome. 

#### No Extreme Outliers

```{r outliers}
# Fit logistic regression model with log-transformed predictors
model_vif <- glm(status ~ age_log + tumor_size_log + rn_examined_log + grade + x6th_stage + race, 
                 data = model_data_3, family = binomial)

# Calculate Cook's Distance
cooksD <- cooks.distance(model_vif)

# Plot Cook's Distance
plot(cooksD, pch = 20, main = "Cook's Distance for Outlier Detection")
abline(h = 4 / nrow(model_data_3), col = "red")

# Identify influential observations
influential_obs <- which(cooksD > 4 / nrow(model_data_3))

# Build Model
model_no_outliers <- glm(status ~ age_log + tumor_size_log + rn_examined_log + grade + x6th_stage + race, 
                         data = model_data_3[-influential_obs, ], family = binomial)

model_robust <- glmrob(status ~ age_log + tumor_size_log + rn_examined_log + grade + x6th_stage + race,
                       data = model_data_3, family = binomial, method = "Mqle")

# Compare summary statistics
summary(model_vif)  # Original model
summary(model_no_outliers)  # Model without influential points
summary(model_robust)
```
In large datasets, Cook's Distance*can flag numerous observations as influential, often due to large sample size. These flagged points may represent true variability in the population rather than errors.
The code results indicate that removing influential points harms the model, likely by disrupting essential data structure. Coefficients for variables like `grade`, `x6th_stage`, and `race` became unstable and unreliable, with extreme standard errors.  
In contrast, the robust logistic regression* (`model_robust`) provided stable and reliable estimates while effectively mitigating the impact of influential observations. Key predictors, such as `age_log`, `rn_examined_log`, `grade`, `x6th_stage`, and `race`, retained coefficients similar to the standard logistic regression.  
The robustness weights further show that about 12% of the data (482 out of 4024 observations) had reduced influence, while the majority (~88%, or 3542 observations) remained largely unaffected with weights close to 1. This approach accounts for influential points by down-weighting their impact, ensuring the model is less sensitive to extreme values without the need to remove data.

#### Independence of Errors
Since there are no group IDs, the independence assumption is satisfied.


